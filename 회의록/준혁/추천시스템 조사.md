## 들어가기에 앞서

<br>

### KoBERT란?

- **KoBERT**는 SKT에서 만든 **한국어 특화 BERT 모델**
- 문장을 입력하면 문장의 의미를 담은 **고정 크기 벡터(768차원)**로 변환
- 예를 들어, `"용감한 토끼가 모험을 떠났다"` → 고정 크기 벡터 변환


<br>


### 임베딩(Embedding)이란?

**임베딩**은 글자, 단어, 문장과 같은 **텍스트 데이터를 숫자 벡터(= 수치)**로 바꾸는 작업 
컴퓨터는 텍스트를 직접 이해하지 못하므로, 의미를 가진 수치화된 벡터로 바꿔줘야 합니다.

예: `"용감한 토끼"` → [0.03, -0.15, 0.41, ..., 0.02] (총 768차원의 숫자 벡터)

이런 **벡터 형태의 표현**을 통해:
- 텍스트 간 **유사도**를 계산할 수 있고 (코사인 유사도)
- 의미가 **비슷한 문장**끼리는 벡터 값도 유사

<br>


### 코사인 유사도란?

> **두 벡터가 이루는 각도의 코사인을 통해 유사도를 측정**하는 방식입니다.
- 두 벡터의 **크기(길이)는 무시**하고,
- **방향이 얼마나 비슷한지(=내용이 비슷한지)**를 보는것

$$\text{Cosine Similarity} = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \times \|\vec{B}\|}$$

- $\vec{A} \cdot \vec{B}$: 두 벡터의 내적 (dot product)
- $\|\vec{A}\|,|\vec{B}\|$: 각 벡터의 크기 
- 결과는 **-1 ~ 1 사이**의 값
    - **1이면 완전히 같은 방향**
    - **0이면 직각 (완전히 무관한 벡터)**
    - **-1이면 완전히 반대 방향**


<br>

## 1. KoBERT 임베딩 기반 추천 (컨텐츠 기반 추천)

### 개념

- 동화의 **줄거리(summary)** 또는 **문단(text)** 내용을 **KoBERT**로 임베딩
- 임베딩 간의 **코사인 유사도**를 계산하여 **비슷한 내용을 가진 동화** 추천

<br>

### 구현 흐름

1. 모든 동화의 줄거리 → KoBERT 임베딩 (768차원 벡터)
2. `story_id` 기준 벡터 저장 (PostgreSQL + pgvector 또는 벡터DB)
3. 기준 동화와의 **코사인 유사도** 기반으로 유사한 동화 top-N 추출
4. **연령대 / 장르 필터링** 적용
5. 추천 결과 API 제공 (`/api/recommend/similar?story_id=1`)
    
### 장점
- 사용자 데이터가 없어도 추천 가능 (**콜드스타트에 강함**)
- 컨텐츠 기반으로 **사용자 취향에 맞는 유사 동화 추천**
- 출시된지 얼마 안된 동화도 노출 가능
    
### 단점
- **항상 비슷한 스타일만 추천** → 다양성 부족
- 임베딩 품질과 벡터 연산 비용 이슈 (미리 계산 필요)
- 사용자간 관계를 고려하지 못함

<br>

### 구현
1. 필요 라이브러리 설치
``` bash
pip install transformers torch
```

<br>

2. KoBERT 모델과 토크나이저 불러오기
```python
from transformers import BertModel, BertTokenizer
import torch

# KoBERT 모델 로드
tokenizer = BertTokenizer.from_pretrained('skt/kobert-base-v1')
model = BertModel.from_pretrained('skt/kobert-base-v1')
model.eval()  # 평가모드
```

<br>

3. 텍스트(요약 or 줄거리)를 벡터(임베딩)로 변환
```python
def get_kobert_embedding(text):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] 임베딩 사용
    return cls_embedding.squeeze().numpy()  # numpy array로 변환

```


<br>

4. 예시 데이터 → 임베딩 생성
```python
story_list = [
    "한 마리 토끼가 용감하게 모험을 떠나는 이야기",
    "숲속 친구들과 우정을 나누는 따뜻한 이야기",
    "탐험을 떠난 토끼가 새로운 세상을 발견하는 이야기",
]

story_embeddings = [get_kobert_embedding(text) for text in story_list]

```

<br>

5. 코사인 유사도 계산 (어떤 동화가 비슷한지)
```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# 예: 첫 번째 동화와 다른 동화들 간 유사도 비교
target_index = 0
target_vec = story_embeddings[target_index].reshape(1, -1)

# 모든 동화와의 유사도 계산
similarities = cosine_similarity(target_vec, story_embeddings)[0]

# 출력
for idx, score in enumerate(similarities):
    print(f"{idx}번 동화: 유사도 {score:.4f}")

```


<br>


6. 예시
``` python
def recommend_similar_story(target_text, all_stories, top_n=3):
    target_vec = get_kobert_embedding(target_text).reshape(1, -1)
    story_vectors = [get_kobert_embedding(s) for s in all_stories]
    sims = cosine_similarity(target_vec, story_vectors)[0]
    
    # 유사도 높은 순 정렬
    ranked = sorted(enumerate(sims), key=lambda x: -x[1])
    
    return [(all_stories[i], round(score, 4)) for i, score in ranked[:top_n]]



# 유저가 좋아하는 동화
recommend_similar_story("토끼가 숲을 여행하는 이야기", story_list)


# 결과
[
    ('탐험을 떠난 토끼가 새로운 세상을 발견하는 이야기', 0.9023),
    ('한 마리 토끼가 용감하게 모험을 떠나는 이야기', 0.8821),
    ('숲속 친구들과 우정을 나누는 따뜻한 이야기', 0.7023)
]


```


<br>

## 2. 협업 필터링 기반 추천 (사용자 행동 기반)

### 개념

- 나와 **비슷한 동화를 좋아한 사용자들**이 **좋아한 다른 동화**를 추천
- `Like` 테이블을 기반으로 유저 간 유사성 계산
    

### 구현 흐름

1. 내가 좋아한 동화를 **다른 사용자들도 좋아했는지** 확인
2. **유사 사용자 Top-N** 추출 (공통 좋아요 수 or 유사도)
3. 그들이 좋아한 동화 중 **내가 안 본 동화** 추천
4. 연령대/장르 필터링
5. 추천 결과 API 제공 (`/api/recommend/personal?user_id=1`)
    

### 장점
- **개인화 추천** 효과가 뛰어남
- 다양한 동화가 추천됨 (**의외성, 다양성 ↑**)
- 내용 분석 없이도 추천 가능
    

### 단점
- **신규 사용자/동화**에 약함 (**콜드스타트** 문제)
- 사용자 수 많아지면 **확장성 이슈**
- 연령대 반영 어려움 (별도 필터 필요)
    


<br>

### 구현

#### 가정: ERD 기반 테이블 구조
- `User(user_id)`
- `Story(story_id, title, ...)`
- `Like(user_id, story_id)`: 사용자와 동화 간 N:M 매핑
    

<br>

#### 협업 필터링 추천 흐름 요약
1. 특정 사용자가 좋아요한 동화 찾기
2. 그 동화를 좋아요한 **다른 사용자들** 찾기
3. 그들이 좋아요한 **다른 동화들 중 내가 안 본 것** 찾기
4. **좋아요 수 기준**으로 랭킹 후 추천

<br>

```python
import pandas as pd

# 예시 Like 데이터: user_id, story_id
likes = pd.DataFrame([
    {'user_id': 1, 'story_id': 101},
    {'user_id': 1, 'story_id': 102},
    {'user_id': 2, 'story_id': 101},
    {'user_id': 2, 'story_id': 103},
    {'user_id': 3, 'story_id': 104},
    {'user_id': 3, 'story_id': 102},
    {'user_id': 4, 'story_id': 103},
    {'user_id': 4, 'story_id': 104},
    {'user_id': 5, 'story_id': 101},
])

def recommend_user(user_id, likes_df, top_n=3):
    # 1. 내가 좋아요한 동화
    my_likes = set(likes_df[likes_df['user_id'] == user_id]['story_id'])

    # 2. 다른 사용자 중, 내가 좋아요한 동화를 같이 좋아한 사람
    other_users = likes_df[(likes_df['story_id'].isin(my_likes)) & (likes_df['user_id'] != user_id)]

    # 3. 그 사용자들이 좋아요한 다른 동화들
    candidate_stories = likes_df[
        (likes_df['user_id'].isin(other_users['user_id'])) &
        (~likes_df['story_id'].isin(my_likes))
    ]

    # 4. 추천 우선순위: 동화별 좋아요 수 기준
    recommendation = candidate_stories['story_id'].value_counts().head(top_n).reset_index()
    recommendation.columns = ['story_id', 'like_count']

    return recommendation

# 사용자 1번에게 추천
print(recommend_user(1, likes))


# 5. 결과
   story_id  like_count
0       103           2
0       104           1



```



<br>


## 3. 장단점 요약
* 컨텐츠 기반 추천과 협업 필터링 기반 추천은 각각 장단점이 있으므로 이 두가지를 결합하는 전략으로 접근하는것이 좋습니다.

| 방법            | 핵심 원리                    | 장점                      | 단점                   |
| ------------- | ------------------------ | ----------------------- | -------------------- |
| KoBERT 임베딩 기반 | 줄거리 등 텍스트 → 벡터화 → 유사도 비교 | 컨텐츠 의미 기반, 새 사용자 문제에 강함 | 계산 비용 큼, 취향 반영 약함    |
| 협업 필터링 기반     | 나와 비슷한 사용자 → 그들이 좋아한 항목  | 사용자 취향 반영, 개인화 강함       | 콜드 스타트 문제, 신규 콘텐츠 약함 |

<br>


## 4. 결합 전략

### 가중 합 방식 (Weighted Hybrid)
* 두 점수 시스템을 각각 계산한 뒤, 가중치를 줘서 결합

``` 
최종 점수 = α * 협업 필터링 점수 + (1 - α) * 임베딩 유사도 점수
```

- `α`는 추천 목적에 따라 조정 가능 (예: `0.6`)
- 사용자 행동이 충분히 쌓였으면 α ↑, 콘텐츠 기반 위주면 α ↓


<br>

```python
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

# 예시: 협업 필터링 점수
collab_scores = pd.DataFrame({
    'story_id': [101, 102, 103],
    'score_collab': [0.9, 0.7, 0.3]
})

# 예시: KoBERT 유사도 점수 (cosine similarity)
content_scores = pd.DataFrame({
    'story_id': [101, 102, 103],
    'score_kobert': [0.6, 0.8, 0.2]
})

# 합치기
merged = pd.merge(collab_scores, content_scores, on='story_id')

# 가중 평균
alpha = 0.6
merged['final_score'] = alpha * merged['score_collab'] + (1 - alpha) * merged['score_kobert']

# 정렬
merged = merged.sort_values(by='final_score', ascending=False)
print(merged[['story_id', 'final_score']])

```


<br>

## 5. DB 구조 고려 시 저장 방식
| 테이블                            | 용도                                  |
| ------------------------------ | ----------------------------------- |
| `story`                        | 동화 텍스트, 요약, 임베딩 벡터 저장 -> vectorDB필요 |
| `like`                         | 사용자 행동 (협업 필터링용)                    |
| `user_recommendation` (필요시 추가) | 사용자별 추천 이력 기록 (스토리ID, 점수 등)         |

<br>

<br>

## 6. 서비스 적용 예시
- FastAPI/Django API로 `/recommend?user_id=1` 엔드포인트 제공
- 내부에서 ① 협업 점수, ② 임베딩 유사도, ③ 가중 평균 → JSON 반환


<br>

## 7. 추가 고도화 방안
|방법|설명|
|---|---|
|가중치 개인화|사용자별 α 조정 (ex. 나이, 이용 패턴 따라)|
|최근 선호 반영|`최근 본 동화`에 더 높은 가중치 부여|
|장르 기반 필터링|추천 결과에서 `UserGenre` 일치율 우선순위 고려|


<br>


### 참고
KoBERT : https://github.com/SKTBrain/KoBERT
구글 머신러닝 추천시스템  : https://developers.google.com/machine-learning/recommendation/overview/types?hl=ko
